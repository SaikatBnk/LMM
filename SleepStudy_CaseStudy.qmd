---
title: "SleepStudy_CaseStudy"

format: html
editor: visual
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(error = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

```

## Load Library:

```{r}
library(tidyverse) # data wrangling and visualization
library(lme4)      # "golden standard" for mixed-effects modelling in R (no p-values)
library(lmerTest)  # p-values for MEMs based on the Satterthwaite approximation
library(psycho)    # mainly for an "analyze()" function
library(broom)     # for tidy results
library(knitr)     # beautifying tables
library(sjPlot)    # for visualising MEMs
library(effects)   # for visualising MEMs
library(report)    # for describing models
library(emmeans)   # for post-hoc analysis
library(Matrix)    # install in console should be Version 1.6-5
```

Know more about the data:

```{r}
?sleepstudy
```

A good way to start every analysis is to plot the data. Here is data for a single subject (308).

```{r}
just_308 <- sleepstudy %>%
  filter(Subject == "308")

ggplot(just_308, aes(x = Days, y = Reaction)) +
  geom_point() +
  scale_x_continuous(breaks = 0:9)
```

Plot below, which shows data for all 18 subjects:

```{r}
ggplot(sleepstudy, aes(x = Days, y = Reaction)) +
  geom_point() +
  scale_x_continuous(breaks = 0:9) +
  facet_wrap(~Subject)
```

## **How to model these data?**

Remove from the dataset observations where `Days` is coded `0` or `1`, and then make a new variable `days_deprived` from the `Days` variable so that the sequence starts at day 2, with day 2 being re-coded as day 0, day 3 as day 1, day 4 as day 2, etc. This new variable now tracks the number of days of sleep deprivation. Store the new table as `sleep2`.

```{r}
sleep2 <- sleepstudy %>%
  filter(Days >= 2L) %>%
  mutate(days_deprived = Days - 2L)
```

It is always a good idea to double check that the code works as intended. First, look at it:

```{r}
head(sleep2)
```

And check that `Days` and `days_deprived` match up.

```{r}
sleep2 %>%
  count(days_deprived, Days)
```

Now let's re-plot the data looking at just these eight data points from Day 0 to Day 7.

```{r}
ggplot(sleep2, aes(x = days_deprived, y = Reaction)) +
  geom_point() +
  scale_x_continuous(breaks = 0:7) +
  facet_wrap(~Subject) +
  labs(y = "Reaction Time", x = "Days deprived of sleep (0 = baseline)")
```

We can apply 3 different approaches:

1.  **complete pooling**, 
2.  **no pooling**, and 
3.  **partial pooling**.

### **Complete pooling: One size fits all**

The **complete pooling** approach is a "one-size-fits-all" model: it estimates a single intercept and slope for the entire dataset, ignoring the fact that different subjects might vary in their intercepts or slopes. If that sounds like a bad approach, it is.

```{r}
cp_model <- lm(Reaction ~ days_deprived, sleep2)

summary(cp_model)
```

According to this model, the predicted mean response time on Day 0 is about 268 milliseconds, with an increase of about 11 milliseconds per day of deprivation, on average. We can't trust the standard errors for our regression coefficients, however, because we are assuming that all of the observations are independent (technically, that the residuals are). However, we can be pretty sure this is a bad assumption.

Let's add the model predictions to the graph that we created above. We can use [`geom_abline()`](https://ggplot2.tidyverse.org/reference/geom_abline.html) to do so

```{r}
coef(cp_model)
```

```{r}
ggplot(sleep2, aes(x = days_deprived, y = Reaction)) +
  geom_abline(intercept = coef(cp_model)[1],
              slope = coef(cp_model)[2],
              color = 'blue') +
  geom_point() +
  scale_x_continuous(breaks = 0:7) +
  facet_wrap(~Subject) +
  labs(y = "Reaction Time", x = "Days deprived of sleep (0 = baseline)")
```

The model fits the data badly. We need a different approach.

### **No pooling**

Pooling all the information to get just one intercept and one slope estimate is inappropriate. Another approach would be to fit separate lines for each participant. This means that the estimates for each participant will be completely uninformed by the estimates for the other participants. In other words, we can separately estimate 18 individual intercept/slope pairs.

This model could be implemented in two ways: (1) by running separate regressions for each participant or (2) by running fixed-effects regression.

Running separate regressions for each participant

```{r}
sleep2 %>% pull(Subject) %>% is.factor()
```

```{r}
np_model <- lm(Reaction ~ days_deprived + Subject + days_deprived:Subject,
               data = sleep2)

summary(np_model)
```

To get population estimates, we could introduce a second stage of analysis where we calculate means of the individual intercepts and slopes. Let's use the model estimates to calculate the intercepts and slopes for each subject.

```{r}
all_intercepts <- c(coef(np_model)["(Intercept)"],
                    coef(np_model)[3:19] + coef(np_model)["(Intercept)"])

all_slopes  <- c(coef(np_model)["days_deprived"],
                 coef(np_model)[20:36] + coef(np_model)["days_deprived"])

ids <- sleep2 %>% pull(Subject) %>% levels() %>% factor()

# make a tibble with the data extracted above
np_coef <- tibble(Subject = ids,
                  intercept = all_intercepts,
                  slope = all_slopes)

np_coef
```

Let's see how well this model fits our data.

```{r}
ggplot(sleep2, aes(x = days_deprived, y = Reaction)) +
  geom_abline(data = np_coef,
              mapping = aes(intercept = intercept,
                            slope = slope),
              color = 'blue') +
  geom_point() +
  scale_x_continuous(breaks = 0:7) +
  facet_wrap(~Subject) +
  labs(y = "Reaction Time", x = "Days deprived of sleep (0 = baseline)")
```

This is much better than the complete pooling model. If we want to test the null hypothesis that the fixed slope is zero, we could do using a one-sample test.

```{r}
np_coef %>% pull(slope) %>% t.test()
```

This tells us that the mean slope of 11.435 is significantly different from zero, t(17) = 6.197, p<.001.

### **Partial pooling using mixed-effects models**

Neither the complete or no-pooling approach is satisfactory. It would be desirable to improve our estimates for individual participants by taking advantage of what we know about the other participants. This will help us better distinguish signal from error for each participant and improve generalization to the population. As the web app below will show, this becomes particularly important when we have unbalanced or missing data.

Let's fit the model, storing the result in object `pp_mod`.

```{r}
pp_mod <- lmer(Reaction ~ days_deprived + (days_deprived | Subject), sleep2)

summary(pp_mod)
```

## **Interpreting `lmer()` output and extracting estimates**

Fixed effect:

```{r}
fixef(pp_mod)
```

The standard errors give us estimates of the variability for these parameters due to sampling error.

You could use these to calculate the t�-values or derive confidence intervals. Extract them using `vcov(pp_mod)` which gives a variance-covariance matrix (*not* the one associated with the random effects), pull out the diagonal using [`diag()`](https://rdrr.io/r/base/diag.html) and then take the square root using [`sqrt()`](https://rdrr.io/r/base/MathFun.html).

```{r}
sqrt(diag(vcov(pp_mod)))

# OR, equivalently using pipes:
# vcov(pp_mod) %>% diag() %>% sqrt()
```

You can get confidence intervals for the estimates using [`confint()`](https://rdrr.io/r/stats/confint.html) (this technique uses the *parametric bootstrap*). 

```{r}
confint(pp_mod)
```

### **Random effects**

Some additional analysis from:

<https://yury-zablotski.netlify.app/post/mixed-effects-models-1/>

```{r}
boxplot(Reaction ~ Subject,
col=c("white","lightgray"),data = sleepstudy)
```

Is the random effect important? Hell, yeah! And there are two ways to prove that:

1.  The amount of variance explained by the *subject* is huge = 3982. How do we know that this number is huge? We can divide the variance for the *subject* by the total variance (variance of the random effect + residuals).

    ```{r}
    Variance_Explained = (958.35/(958.35 + 45.78 + 651.6))*100

    Variance_Explained 
    ```

```{r}
coef(pp_mod)
```

```{r}
library(lattice) 
dotplot(ranef(pp_mod, condVar=T))
```
